{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#抓取新聞連結\n",
    "def getlink(script):\n",
    "    arry = [] \n",
    "    global count\n",
    "    global pkcount \n",
    "    apl = 'apl'\n",
    "    soup = BeautifulSoup(res.text)\n",
    "    for a_tag in soup.select('#result h2 a'):\n",
    "        alink = a_tag.get('href').strip()\n",
    "        arry.append(alink)\n",
    "        for elelink in arry:\n",
    "            num = str(pkcount)\n",
    "            if pkcount < 10:\n",
    "                num = str(0) * 4 + num\n",
    "            elif pkcount < 100:     \n",
    "                num = str(0) * 3 + num\n",
    "            elif pkcount < 1000:     \n",
    "                num = str(0) * 2 + num\n",
    "            elif pkcount < 10000:     \n",
    "                num = str(0) * 1 + num\n",
    "            pk = apl + num\n",
    "        links = {'link':alink,'area':area,'page':page,'count':count,'pk':pk}\n",
    "        linkary.append(links)\n",
    "        count += 1\n",
    "        pkcount += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from datetime import *\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "#12個行政區['中正區','大同區','中山區','松山區','大安區','萬華區','信義區','士林區','北投區','內湖區','南港區','文山區']\n",
    "ary = ['中正區','大同區','中山區','松山區','大安區','萬華區','信義區','士林區','北投區','內湖區','南港區','文山區']\n",
    "links = {'link':'','area':'','page':'','count':'','pk':''}\n",
    "linkary =[]\n",
    "    \n",
    "#系統時間------------------------------\n",
    "today = date.today()\n",
    "year = today.year\n",
    "month = today.month\n",
    "day = today.day\n",
    "if month < 10:\n",
    "    month = \"0\" + str(month)\n",
    "if day < 10:\n",
    "    day = \"0\" + str(day)\n",
    "year = str(year)\n",
    "month = str(month)\n",
    "day = str(day)\n",
    "#--------------------------------------\n",
    "\n",
    "headers={\n",
    "'Origin':'http://search.appledaily.com.tw',\n",
    "'Referer':'http://search.appledaily.com.tw/appledaily/search',\n",
    "'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36' \n",
    "}\n",
    "pkcount = 1    # pkcount = 1\n",
    "for area in ary:\n",
    "    count = 1\n",
    "    for page in range(1,12):\n",
    "        payload = {\n",
    "        'searchMode':'',\n",
    "        'searchType':'text',\n",
    "        'ExtFilter':'',\n",
    "        'sorttype':'1',\n",
    "        'keyword':area,\n",
    "        'rangedate':'[20030502 TO'+year+month+day+'999:99'']',\n",
    "        'totalpage':'',\n",
    "        'page':page\n",
    "        }\n",
    "\n",
    "        res = requests.post('http://search.appledaily.com.tw/appledaily/search',headers=headers,data=payload)\n",
    "        script = res.text\n",
    "        getlink(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "data1 = pandas.DataFrame(linkary)\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#輸出成CSV\n",
    "data1.to_csv('apl_link.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#讀取CSV檔\n",
    "import csv\n",
    "linkary = []\n",
    "with open('apl_linktst.csv',) as csvfile:   #apl_linktst\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        linkary.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #取得人氣\n",
    "# def getlike(soup): \n",
    "#     if soup.select('.urcc a') is not None:\n",
    "#         reele = soup.select('.urcc a')[0].text\n",
    "#         print soup.select('.urcc a')\n",
    "#         rdrx = re.search('(\\d+)',reele)\n",
    "#         likes = rdrx.group(1)\n",
    "#     elif soup.select('.function_icon.clicked') is not None:\n",
    "#         reele = select('.function_icon.clicked')[0].text\n",
    "#         rdrx = re.search('(\\d+)',reele)\n",
    "#         likes = rdrx.group(1)\n",
    "#     else:\n",
    "#         likes = \"0\"\n",
    "#     return likes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#抓取新聞內文\n",
    "import requests\n",
    "import time\n",
    "import re  # --\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = { \n",
    "'Origin':'http://search.appledaily.com.tw',\n",
    "'Referer':'http://search.appledaily.com.tw/appledaily/search',\n",
    "'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36'\n",
    "}\n",
    "\n",
    "for ele in linkary:\n",
    "    url = ele['link']\n",
    "    text = ''\n",
    "    while True:\n",
    "        try:\n",
    "            res = requests.get(url,headers=headers)\n",
    "            res.text\n",
    "            soup = BeautifulSoup(res.text)\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "            \n",
    "    try:\n",
    "        ele['title'] = soup.select('hgroup #h1')[0].text.strip()\n",
    "        ele['date'] = soup.select('.gggs time')[0]['datetime']\n",
    "        for p_tag in soup.select('#summary'):\n",
    "            text += p_tag.text.strip()\n",
    "        ele['content'] = text\n",
    "#         ele['likes'] = getlike(soup)\n",
    "#---------------------------------------------------------------        \n",
    "        if soup.select('.function_icon.clicked') is not None:\n",
    "            reele = select('.function_icon.clicked')[0].text\n",
    "            rdrx = re.search('(\\d+)',reele)\n",
    "            likes = rdrx.group(1)\n",
    "        else:\n",
    "            likes = \"0\"\n",
    "        ele['likes'] = likes\n",
    "#---------------------------------------------------------------\n",
    "    except:\n",
    "        print \"This link has some problems!\"\n",
    "        print ele['link'], ele['area'], ele['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#新聞內文進入DataFrame\n",
    "import pandas\n",
    "data2 = pandas.DataFrame(linkary)\n",
    "\n",
    "# 移除多餘的 column\n",
    "# del data2['column name']\n",
    "apl_ok = data2.drop(data2.columns[[0]], axis=1)\n",
    "apl_ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#輸出完成品\n",
    "apl_ok.to_csv('apl.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "headers={\n",
    "'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.80 Safari/537.36'\n",
    "}\n",
    "res = requests.get('http://www.appledaily.com.tw/appledaily/article/headline/20151105/36881669/applesearch/%E8%B2%B8%E6%AC%BE%E6%88%90%E6%95%B8%E4%BD%8E%E8%BF%B7%E3%80%81%E5%B1%8B%E4%B8%BB%E6%9C%89%E6%84%9F%E9%99%8D%E5%83%B9%E3%80%80%E4%BD%95%E6%99%82%E8%A9%B2%E8%B2%B7%E6%88%BF%EF%BC%9F',headers=headers)\n",
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "if len(soup.select('.urcc a')) > 0:\n",
    "    reele = soup.select('.urcc a')[0].text\n",
    "    print \"1\"\n",
    "    rdrx = re.search('(\\d+)',reele)\n",
    "    likes = rdrx.group(1)\n",
    "elif len(soup.select('.fntss a')) > 0:\n",
    "    print \"2\"\n",
    "    reele = soup.select('.fntss a')[0].text\n",
    "    print reele\n",
    "    rdrx = re.search('(\\d+)',reele)\n",
    "    likes = rdrx.group(0)\n",
    "else:\n",
    "    likes = \"0\"\n",
    "    print likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "print soup.select('.function_icon.clicked')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
