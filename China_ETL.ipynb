{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "res=requests.get('http://www.chinatimes.com/newspapers/20150812000136-260204')\n",
    "print res.text\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.text)\n",
    "print soup.select('title')[2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs \n",
    "import requests\n",
    "ary = ['松山區']\n",
    "driver = webdriver.Firefox()\n",
    "for area in ary:\n",
    "    driver.get('http://www.chinatimes.com/search/result.htm?q=' + area)\n",
    "    soup0 = driver.page_source\n",
    "    #print bs(soup0)\n",
    "    soup = bs(soup0)\n",
    "    page_format = 'http://www.chinatimes.com/search/result.htm?q=' + area +'#gsc.tab=0&gsc.q=' + area+'&gsc.page={}'\n",
    "    for page in range(1, 8):\n",
    "        pageurl = page_format.format(page)\n",
    "        for s in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "            if s.get('href') is not None:\n",
    "                ad = str(s.get('href'))\n",
    "                if 'newspapers' in ad:\n",
    "                    print ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get('http://www.chinatimes.com/search/result.htm?q=士林區')\n",
    "res = driver.page_source\n",
    "soup = BeautifulSoup(res)\n",
    "\n",
    "page_format = 'http://www.chinatimes.com/search/result.htm?q=' + area +'#gsc.tab=0&gsc.q=' + area+'&gsc.page={}'\n",
    "for page in range(1, 8):\n",
    "    pageurl = page_format.format(page)\n",
    "    for title in soup.select('.gsc-thumbnail-inside'):\n",
    "        print title.select('a')[0].text\n",
    "\n",
    "    \n",
    "#title = soup.select('.gsc-thumbnail-inside')[0].text\n",
    "#title2 = title.select('a')\n",
    "#print title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get('http://www.chinatimes.com/search/result.htm?q=士林區')\n",
    "res = driver.page_source\n",
    "soup = BeautifulSoup(res)\n",
    "for link in soup.select('.gsc-thumbnail-inside'):\n",
    "#     if link.get('a')[0]['data-ctorig'] is not None:\n",
    "#         print link.get('a')[0]['data-ctorig']\n",
    "    link = link.select('a')['data-ctorig']\n",
    "    print link\n",
    "    \n",
    "#row_link = soup.select('.gsc-thumbnail-inside a')[0]['data-ctorig']\n",
    "#for row_link\n",
    "\n",
    "\n",
    "#not here\n",
    "#  for s in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "#             if s.get('href') is not None:\n",
    "#                 ad = str(s.get('href'))\n",
    "#                 if 'newspapers' in ad:\n",
    "#                     print ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs \n",
    "import requests\n",
    "ary = ['松山區']\n",
    "driver = webdriver.Firefox()\n",
    "for area in ary:\n",
    "    driver.get('http://www.chinatimes.com/search/result.htm?q=' + area)\n",
    "    soup0 = driver.page_source\n",
    "    #print bs(soup0)\n",
    "    soup = bs(soup0)\n",
    "    page_format = 'http://www.chinatimes.com/search/result.htm?q=' + area +'#gsc.tab=0&gsc.q=' + area+'&gsc.page={}'\n",
    "    for page in range(1, 2):\n",
    "        pageurl = page_format.format(page)\n",
    "        for news_link in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "            if news_link.get('href') is not None:\n",
    "                href_link = str(news_link.get('href'))\n",
    "                if 'newspapers' in href_link:\n",
    "#                     print href_link\n",
    "                    driver.get(href_link)\n",
    "                    res2 = driver.page_source\n",
    "#                     print res2\n",
    "                    soup2 = bs(res2)\n",
    "                    detailtext = soup2.select('article p')\n",
    "                    for i in range (0,len(detailtext)):\n",
    "                        print detailtext[i].text.strip()\n",
    "                        print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_link = driver.get(href_link)\n",
    "for i in row_link:\n",
    "    res2 = driver.page_source\n",
    "    soup2 = bs(res2)\n",
    "    print soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(href_link)\n",
    "res2 = driver.page_source\n",
    "soup2 = bs(res2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "artm = soup2.select('article p')\n",
    "for i in range (0,len(artm)):\n",
    "    print artm[i].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "driver = webdriver.Firefox()\n",
    "ary = ['松山區']    #12個行政區\n",
    "for area in ary:\n",
    "    driver.get('http://www.chinatimes.com/search/result.htm?q=' + area)\n",
    "    res_text = driver.page_source\n",
    "    soup = BeautifulSoup(res_text)\n",
    "    page_format = 'http://www.chinatimes.com/search/result.htm?q=' + area + '#gsc.tab=0&gsc.q=' + area + '&gsc.page={}'\n",
    "    for page in range(1,2):    #頁數\n",
    "        page_url = page_format.format(page)\n",
    "        driver.get(page_url)\n",
    "        res_text2 = driver.page_source\n",
    "        soup2 = BeautifulSoup(res_text2)\n",
    "        for bid in soup2.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "            if bid.get('href') is not None:\n",
    "                str_bid = str(bid.get('href'))\n",
    "                if 'newspapers' in str_bid:\n",
    "                    print str_bid\n",
    "                    driver.get(str_bid)\n",
    "                    res2 = driver.page_source\n",
    "                    soup2 = BeautifulSoup(res2)\n",
    "                    title = soup2.select('article h1')[0].text\n",
    "                    time = soup2.select('article time')[0]['datetime']\n",
    "                    print title, time \n",
    "                    print str_bid\n",
    "                    for p_bid in soup2.select('article p')[0:]:\n",
    "                        print p_bid.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pan_ary = []\n",
    "for \n",
    "    pan_ary.append({\n",
    "        'title':title,\n",
    "        'time':time,\n",
    "        'link':str_bid,\n",
    "        'text':p_bid.text.strip()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "df = pandas.DataFrame(pan_ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.index.names = \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = soup2.select('article h1')[0].text\n",
    "time = soup2.select('article time')[0]['datetime']\n",
    "print title, time, str_bid\n",
    "for p_bid in soup2.select('article p')[0:]:\n",
    "    print p_bid.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "driver = webdriver.Firefox()\n",
    "ary = ['士林區']    #12個行政區\n",
    "for area in ary:\n",
    "    driver.get('http://www.chinatimes.com/search/result.htm?q=' + area) #Firefox開啟松山區新聞選單頁面\n",
    "    res_text = driver.page_source\n",
    "    soup = BeautifulSoup(res_text)\n",
    "    page_format = 'http://www.chinatimes.com/search/result.htm?q=' + area + '#gsc.tab=0&gsc.q=' + area + '&gsc.page={}'\n",
    "    for page in range(1,3):    #ex:1頁 ==> (1,2)\n",
    "        page_url = page_format.format(page)\n",
    "        driver.get(page_url)\n",
    "        res_text2 = driver.page_source\n",
    "        soup2 = BeautifulSoup(res_text2)\n",
    "        for bid in soup2.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "            if bid.get('href') is not None:\n",
    "                str_bid = str(bid.get('href'))\n",
    "                if 'newspapers' in str_bid:\n",
    "#                      print str_bid                    \n",
    "                    driver.get(str_bid)\n",
    "                    res3 = driver.page_source\n",
    "                    soup3 = BeautifulSoup(res3)\n",
    "                    title = soup3.select('article h1')[0].text\n",
    "                    time = soup3.select('article time')[0]['datetime']\n",
    "                    print title, time \n",
    "                    print str_bid\n",
    "                    for p_bid in soup3.select('article p')[0:]:\n",
    "                        print p_bid.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "driver = webdriver.Firefox()\n",
    "ary = ['士林區']    #12個行政區\n",
    "\n",
    "\n",
    "for area in ary:\n",
    "    page_format = 'http://www.chinatimes.com/search/result.htm?q=' + area + '#gsc.tab=0&gsc.q=' + area + '&gsc.page={}'\n",
    "    for page in range(1,3):    #ex:1頁 ==> (1,2)\n",
    "        page_url = page_format.format(page)\n",
    "        driver.get(page_url)\n",
    "        res_text = driver.page_source\n",
    "        soup = BeautifulSoup(res_text)\n",
    "        for bid in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "            if bid.get('href') is not None:\n",
    "                str_bid = str(bid.get('href'))\n",
    "                if 'newspapers' in str_bid:\n",
    "#                     print str_bid\n",
    "                    driver.get(str_bid)\n",
    "                    res_text2 = driver.page_source\n",
    "                    soup2 = BeautifulSoup(res_text2)\n",
    "                    title = soup3.select('article h1')[0].text\n",
    "                    time = soup3.select('article time')[0]['datetime']\n",
    "                    print title, time \n",
    "                    print str_bid\n",
    "                    for p_bid in soup3.select('article p')[0:]:\n",
    "                        print p_bid.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver.get(str_bid)\n",
    "res_text2 = driver.page_source\n",
    "soup2 = BeautifulSoup(res_text2)\n",
    "print soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = soup3.select('article h1')[0].text\n",
    "time = soup3.select('article time')[0]['datetime']\n",
    "print title, time \n",
    "print str_bid\n",
    "for p_bid in soup3.select('article p')[0:]:\n",
    "    print p_bid.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------start from here-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getlink(soup):\n",
    "    for bid in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "        if bid.get('href') is not None:\n",
    "            str_bid = str(bid.get('href'))\n",
    "            if 'newspapers' in str_bid:\n",
    "                print str_bid\n",
    "                res2 = requests.get(str_bid, headers=headers)\n",
    "                soup2 = BeautifulSoup(res2.content)\n",
    "                res2.close()\n",
    "                time.sleep(3)\n",
    "                for zone in soup2.select('.clear-fix'):\n",
    "                    title = soup2.select('h1')[0].text\n",
    "                    times = soup2.select('time')['datetime']\n",
    "                    text =  soup2.select('p')[0].text.strip() \n",
    "                    print title\n",
    "                    print times\n",
    "                    print area\n",
    "                    print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(2)\n",
    "ary = ['中正區']    #12個行政區\n",
    "url = 'http://www.chinatimes.com/search/result.htm?q='+ary[0]\n",
    "pan_ary = []#建字典\n",
    "for area in ary:\n",
    "    for page in range(1,2):    #ex:1頁 ==> (1,2)\n",
    "        page_url = 'http://www.chinatimes.com/search/result.htm?q=' + area + '#gsc.tab=0&gsc.q=' + area + '&gsc.page={}'.format(page)\n",
    "        \n",
    "        driver.get(page_url)\n",
    "        res_text = driver.page_source\n",
    "        soup = BeautifulSoup(res_text)\n",
    "        \n",
    "        headers = {\n",
    "            'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Encoding':'gzip, deflate, sdch',\n",
    "            'Accept-Language':'zh-TW,zh;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "            'Connection':'keep-alive',\n",
    "            'Host':'www.chinatimes.com',\n",
    "            'Referer':url,\n",
    "            'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'\n",
    "        }\n",
    "        getlink(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.chinatimes.com/newspapers/20150513000138-260204\n",
      "http://www.chinatimes.com/newspapers/20150917000470-260102\n",
      "http://www.chinatimes.com/newspapers/20150925000727-260110\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "\n",
    "ary = ['中正區']    #12個行政區\n",
    "pan_ary = []#建字典\n",
    "for area in ary:\n",
    "    for page in range(1,2):    #ex:1頁 ==> (1,2)\n",
    "        if page == 1:\n",
    "            page_url = \"http://www.chinatimes.com/search/result.htm?q=\"+ area\n",
    "            driver.get(page_url)\n",
    "            res_text = driver.page_source\n",
    "            soup = BeautifulSoup(res_text)\n",
    "            driver.find_element_by_xpath(\"//div[@id='___gcse_0']/div/div/div/div[5]/div[2]/div[2]/div/div[2]/div[13]/div/div[{}]\".format(page+1)).click()\n",
    "        else:\n",
    "            res_text = driver.page_source\n",
    "            soup = BeautifulSoup(res_text)\n",
    "            driver.find_element_by_xpath(\"//div[@id='___gcse_0']/div/div/div/div[5]/div[2]/div[2]/div/div[2]/div[13]/div/div[{}]\".format(page+1)).click()\n",
    "        for bid in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "            if bid.get('href') is not None:\n",
    "                str_bid = str(bid.get('href'))\n",
    "                if 'newspapers' in str_bid:\n",
    "                    print str_bid\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "headers = {\n",
    "            'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Encoding':'gzip, deflate, sdch',\n",
    "            'Accept-Language':'zh-TW,zh;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "            'Connection':'keep-alive',\n",
    "            'Host':'www.chinatimes.com',\n",
    "            'Referer':page_url,\n",
    "            'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'\n",
    "        }\n",
    "\n",
    "res2 = requests.get(str_bid)\n",
    "soup2 = BeautifulSoup(res2.text)\n",
    "res2.close()\n",
    "# print soup2.select('article h1')[0].text\n",
    "# print soup2.select('article time')[0]['datetime']\n",
    "# for p_tag in soup2.select('article p'):\n",
    "#     print p_tag.text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "畢業季即將到來，社會新鮮人即將踏入職場，但北市昂貴的房租，成為新鮮人生活上一大考驗。專家建議，新鮮人租金預算最好不要超過月薪1／3，以去年畢業生平均薪資27,152元計算，建議應控制在9,000元左右。其中，大同與萬華區就是租屋首選，即使平均租金較高的信義區，也有二線生活圈可選擇。台灣房屋智庫根據租屋實價資訊顯示，緊鄰信義商圈的忠孝東路5段，每月單坪租金1,887元；金融業群聚的南京東路2段租金1,938元。如果以7～8坪套房計算，月租金介於13,200～15,500元間，根本不是初入社會的新鮮人能負擔的價格。究竟9,000元的月租預算，如何在台北市生存？根據《好房網》資料統計，北市套房每坪平均租金最便宜的前5名，分別為文山、北投、大同、士林和萬華區。但文山、北投距離市區過遠，士林區有些地域又靠近山邊，新鮮人想透過捷運轉乘進入台北市區並不方便，生活機能完善也便利的大同、萬華區，才是新鮮人租屋首選。大同區交通便利大同區位於北市西邊，是北市最早發展的區域，周邊有中山地下街、雙連市場、馬偕醫院、寧夏夜市，商業行為成熟，生活機能發達，區內更有捷運圓山站、民權西路站、雙連站、中山站、大橋頭站，交通便捷；但也因為開發早，社區多為老社區，房價、租金相對來說較為便宜。不過，大同區舊屋翻新案例多，又有捷運題材，部分新物建租金水準已飆高，想在大同區找到便宜的房子，還是有些技巧。《好房網》總編輯吳光中建議，新鮮人可往捷運中山站周邊的華陰街、天津街、太原路尋寶，這些路段皆鄰近長安西路且商圈成熟，下班後可覓食的點也多，對於外食族來說相當方便，若以每月租金9,000元左右的預算，可租到7～8坪的房子，是經濟又實惠的選擇。他還指出，捷運民權西路站也是不錯的選擇，除了交通方便、生活機能完善外，治安相對比中山站附近好。他更將建議範圍縮小到以民權西路站為中心，北至天祥路86巷；南至錦西街，這些都是月租9,000元左右的地段。但吳光中叮嚀，租屋不要只看房子，從捷運站到租屋處最好實際走過一遍，特別是女生，走過才知道沿路上是否有監視器，且早晚環境狀況也會不同，建議早晚都走一遍，確認街道巷弄是否會太暗。他強調，年輕女上班族還是要以安全為最重要考量。萬華區注意環境至於位於北市西南側的萬華區，也是大台北最早發展的區域，曾有一府二鹿三艋舺的美譽；如今卻是北市最老舊社區，街廓雜亂無規畫，加上遊民、流鶯聚集，造成一般人對萬華既定印象不佳，房價相對比其他行政區低。不過，萬華區靠近運龍山寺捷運站及西門站的區塊，交通與生活機能都有一定水準。永慶房屋西門店店長楊証舜建議，新鮮人找屋，可朝捷運龍山寺站附近，康定路以東的桂林路上尋找。他指出，該區為純住宅區，治安跟出入分子相對好，如果新鮮人有交通工具，環河南路2段等交通邊陲地帶，也是不錯的選擇。這些地方8坪大的物件，月租金都在1萬元以下。此外，吳光中也建議，如果新鮮人一定要在北市主要辦公商圈租屋，例如大安、信義、松山、中正區，也可以選擇「二線生活圈」。以大安信義區為例，捷運六張犁站附近的信安街、嘉興街、樂利路；或捷運麟光站的臥龍街。至於松山區的二線生活圈，則以捷運南京三民路附近的塔悠路、撫遠街、寶清街、八德路4段為主。專家表示，這些二線生活圈的房租，肯定比行政區的正中心低，新鮮人不妨多關注，或許有機會撈到寶，在距離公司最近的地方租到好房。台北市套房租屋行情（獨立套房）行政區\t每坪平均租金文山區\t1,238.5北投區\t1,267.5大同區\t1,426.5南港區\t1,437.0士林區\t1,465.3萬華區\t1,486.4內湖區\t1,583.9松山區\t1,679.7中山區\t1,679.8中正區\t1,724.4信義區\t1,749.3大安區\t1,928.6新北市主要租屋區行情（獨立套房）行政區\t每坪平均租金板橋區\t1,217.3三重區\t1,203.6中和區\t1,198.0永和區\t1,297.6新莊區\t1,006.3新店區\t1,263.5汐止區\t1033.21946期《時報周刊》一套原價75元，特價59元。全通路雜誌均內附超商折價券，讓讀者激省1093元。伊林娛樂2015璀璨之星選拔即日起開始報名，獎項超豐富，還有機會獲得LUXGEN百萬SUV車。P.S.由「台灣的良心」林杰樑醫師遺孀譚敦慈所書專欄「元氣慈場」於《時報周刊》精采登場，教導民眾如何從生活中杜絕毒物上身。（訂《時報周刊》送健康暢銷書【溫暖腸道，吃出排便力】，請洽讀者服務專線：0800-000-668。）\n"
     ]
    }
   ],
   "source": [
    "# for ele in soup2.select('article'):\n",
    "s =''\n",
    "for p_tag in soup2.select('article p'):\n",
    "    s = s + p_tag.text.strip()\n",
    "# print soup2.select('article header h1')[0].text\n",
    "# print soup2.select('time')[0]['datetime']\n",
    "# for p_tag in soup2.select('article p'):\n",
    "#     print p_tag.text\n",
    "        \n",
    "#     print ele.select('time')[0]['datetime']\n",
    "#     for p_tag in ele.select('p'):\n",
    "#         print p_tag.text\n",
    "print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getinfo(soup):\n",
    "    for bid in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "        if bid.get('href') is not None:\n",
    "            str_bid = str(bid.get('href'))\n",
    "            if 'newspapers' in str_bid:\n",
    "                print str_bid\n",
    "        res2 = requests.get(str_bid)\n",
    "        soup2 = BeautifulSoup(res2.text)\n",
    "        res2.close()\n",
    "        text = \"\"\n",
    "        for p_tag in soup2.select('article p'):\n",
    "            text = text + p_tag.text.strip()\n",
    "        if is not None:\n",
    "        tt = soup2.select('article header h1')[0].text\n",
    "        pan_ary.append({\n",
    "        \"area\":area,\n",
    "        \"links\":str_bid,\n",
    "        \"title\":tt,\n",
    "        \"times\":soup2.select('time')[0]['datetime'],\n",
    "        \"content\":text\n",
    "        })            \n",
    "            \n",
    "#             print ele.select('h1')[0].text\n",
    "#             print ele.select('time')[0]['datetime']\n",
    "#             for p_tag in ele.select('p'):\n",
    "#                 print p_tag.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.chinatimes.com/newspapers/20150513000138-260204\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-8c4e5559dc06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;34m'User-Agent'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         }\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mgetinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpan_ary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-5644137d5e0c>\u001b[0m in \u001b[0;36mgetinfo\u001b[1;34m(soup)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;34m\"area\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0marea\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;34m\"links\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstr_bid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;34m\"title\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msoup2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'article header h1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[1;34m\"times\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0msoup2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;34m\"content\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "\n",
    "ary = ['中正區']    #12個行政區\n",
    "pan_ary = []#建字典\n",
    "for area in ary:\n",
    "    for page in range(1,2):    #ex:1頁 ==> (1,2)\n",
    "        if page == 1:\n",
    "            page_url = \"http://www.chinatimes.com/search/result.htm?q=\"+ area\n",
    "            driver.get(page_url)\n",
    "            driver.find_element_by_css_selector(\"div.gsc-option-selector\").click()\n",
    "            driver.find_element_by_css_selector(\"div.gsc-option\").click()\n",
    "            res_text = driver.page_source\n",
    "            soup = BeautifulSoup(res_text)\n",
    "            driver.find_element_by_xpath(\"//div[@id='___gcse_0']/div/div/div/div[5]/div[2]/div[2]/div/div[2]/div[13]/div/div[{}]\".format(page+1)).click()\n",
    "        else:\n",
    "            res_text = driver.page_source\n",
    "            soup = BeautifulSoup(res_text)\n",
    "            driver.find_element_by_xpath(\"//div[@id='___gcse_0']/div/div/div/div[5]/div[2]/div[2]/div/div[2]/div[13]/div/div[{}]\".format(page+1)).click()\n",
    "        headers = {\n",
    "            'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Encoding':'gzip, deflate, sdch',\n",
    "            'Accept-Language':'zh-TW,zh;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "            'Connection':'keep-alive',\n",
    "            'Host':'www.chinatimes.com',\n",
    "            'Referer':page_url,\n",
    "            'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'\n",
    "        }\n",
    "        getinfo(soup)\n",
    "driver.close()\n",
    "df = pandas.Dataframe(pan_ary)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"div.gsc-option-selector\"}\nStacktrace:\n    at FirefoxDriver.prototype.findElementInternal_ (file:///c:/users/bigdata/appdata/local/temp/tmpwm8woa/extensions/fxdriver@googlecode.com/components/driver-component.js:10647)\n    at fxdriver.Timer.prototype.setTimeout/<.notify (file:///c:/users/bigdata/appdata/local/temp/tmpwm8woa/extensions/fxdriver@googlecode.com/components/driver-component.js:623)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a740956f8e2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_url\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/search/result.htm?q=%E5%A3%AB%E6%9E%97%E5%8D%80#gsc.tab=0&gsc.q=%E5%A3%AB%E6%9E%97%E5%8D%80&gsc.page=1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_css_selector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div.gsc-option-selector\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_css_selector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"div.gsc-option\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mfind_element_by_css_selector\u001b[1;34m(self, css_selector)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_css_selector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'#foo'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m         \"\"\"\n\u001b[1;32m--> 402\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCSS_SELECTOR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcss_selector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements_by_css_selector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcss_selector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    710\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'[name=\"%s\"]'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m         return self.execute(Command.FIND_ELEMENT,\n\u001b[1;32m--> 712\u001b[1;33m                              {'using': by, 'value': value})['value']\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.pyc\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    203\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.pyc\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mexception_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mUnexpectedAlertPresentException\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m'alert'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"div.gsc-option-selector\"}\nStacktrace:\n    at FirefoxDriver.prototype.findElementInternal_ (file:///c:/users/bigdata/appdata/local/temp/tmpwm8woa/extensions/fxdriver@googlecode.com/components/driver-component.js:10647)\n    at fxdriver.Timer.prototype.setTimeout/<.notify (file:///c:/users/bigdata/appdata/local/temp/tmpwm8woa/extensions/fxdriver@googlecode.com/components/driver-component.js:623)"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoAlertPresentException\n",
    "import unittest, time, re\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "driver.get(base_url + \"/search/result.htm?q=%E5%A3%AB%E6%9E%97%E5%8D%80#gsc.tab=0&gsc.q=%E5%A3%AB%E6%9E%97%E5%8D%80&gsc.page=1\")\n",
    "driver.find_element_by_css_selector(\"div.gsc-option-selector\").click()\n",
    "driver.find_element_by_css_selector(\"div.gsc-option\").click()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
