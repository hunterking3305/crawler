{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "res=requests.get('http://www.chinatimes.com/newspapers/20150812000136-260204')\n",
    "print res.text\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(res.text)\n",
    "print soup.select('title')[2].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs \n",
    "import requests\n",
    "ary = ['松山區']\n",
    "driver = webdriver.Firefox()\n",
    "for area in ary:\n",
    "    driver.get('http://www.chinatimes.com/search/result.htm?q=' + area)\n",
    "    soup0 = driver.page_source\n",
    "    #print bs(soup0)\n",
    "    soup = bs(soup0)\n",
    "    page_format = 'http://www.chinatimes.com/search/result.htm?q=' + area +'#gsc.tab=0&gsc.q=' + area+'&gsc.page={}'\n",
    "    for page in range(1, 8):\n",
    "        pageurl = page_format.format(page)\n",
    "        for s in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "            if s.get('href') is not None:\n",
    "                ad = str(s.get('href'))\n",
    "                if 'newspapers' in ad:\n",
    "                    print ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get('http://www.chinatimes.com/search/result.htm?q=士林區')\n",
    "res = driver.page_source\n",
    "soup = BeautifulSoup(res)\n",
    "\n",
    "page_format = 'http://www.chinatimes.com/search/result.htm?q=' + area +'#gsc.tab=0&gsc.q=' + area+'&gsc.page={}'\n",
    "for page in range(1, 8):\n",
    "    pageurl = page_format.format(page)\n",
    "    for title in soup.select('.gsc-thumbnail-inside'):\n",
    "        print title.select('a')[0].text\n",
    "\n",
    "    \n",
    "#title = soup.select('.gsc-thumbnail-inside')[0].text\n",
    "#title2 = title.select('a')\n",
    "#print title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get('http://www.chinatimes.com/search/result.htm?q=士林區')\n",
    "res = driver.page_source\n",
    "soup = BeautifulSoup(res)\n",
    "for link in soup.select('.gsc-thumbnail-inside'):\n",
    "#     if link.get('a')[0]['data-ctorig'] is not None:\n",
    "#         print link.get('a')[0]['data-ctorig']\n",
    "    link = link.select('a')['data-ctorig']\n",
    "    print link\n",
    "    \n",
    "#row_link = soup.select('.gsc-thumbnail-inside a')[0]['data-ctorig']\n",
    "#for row_link\n",
    "\n",
    "\n",
    "#not here\n",
    "#  for s in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "#             if s.get('href') is not None:\n",
    "#                 ad = str(s.get('href'))\n",
    "#                 if 'newspapers' in ad:\n",
    "#                     print ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs \n",
    "import requests\n",
    "ary = ['松山區']\n",
    "driver = webdriver.Firefox()\n",
    "for area in ary:\n",
    "    driver.get('http://www.chinatimes.com/search/result.htm?q=' + area)\n",
    "    soup0 = driver.page_source\n",
    "    #print bs(soup0)\n",
    "    soup = bs(soup0)\n",
    "    page_format = 'http://www.chinatimes.com/search/result.htm?q=' + area +'#gsc.tab=0&gsc.q=' + area+'&gsc.page={}'\n",
    "    for page in range(1, 2):\n",
    "        pageurl = page_format.format(page)\n",
    "        for news_link in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "            if news_link.get('href') is not None:\n",
    "                href_link = str(news_link.get('href'))\n",
    "                if 'newspapers' in href_link:\n",
    "#                     print href_link\n",
    "                    driver.get(href_link)\n",
    "                    res2 = driver.page_source\n",
    "#                     print res2\n",
    "                    soup2 = bs(res2)\n",
    "                    detailtext = soup2.select('article p')\n",
    "                    for i in range (0,len(detailtext)):\n",
    "                        print detailtext[i].text.strip()\n",
    "                        print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "row_link = driver.get(href_link)\n",
    "for i in row_link:\n",
    "    res2 = driver.page_source\n",
    "    soup2 = bs(res2)\n",
    "    print soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(href_link)\n",
    "res2 = driver.page_source\n",
    "soup2 = bs(res2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "artm = soup2.select('article p')\n",
    "for i in range (0,len(artm)):\n",
    "    print artm[i].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "driver = webdriver.Firefox()\n",
    "ary = ['松山區']    #12個行政區\n",
    "for area in ary:\n",
    "    driver.get('http://www.chinatimes.com/search/result.htm?q=' + area)\n",
    "    res_text = driver.page_source\n",
    "    soup = BeautifulSoup(res_text)\n",
    "    page_format = 'http://www.chinatimes.com/search/result.htm?q=' + area + '#gsc.tab=0&gsc.q=' + area + '&gsc.page={}'\n",
    "    for page in range(1,2):    #頁數\n",
    "        page_url = page_format.format(page)\n",
    "        driver.get(page_url)\n",
    "        res_text2 = driver.page_source\n",
    "        soup2 = BeautifulSoup(res_text2)\n",
    "        for bid in soup2.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "            if bid.get('href') is not None:\n",
    "                str_bid = str(bid.get('href'))\n",
    "                if 'newspapers' in str_bid:\n",
    "                    print str_bid\n",
    "                    driver.get(str_bid)\n",
    "                    res2 = driver.page_source\n",
    "                    soup2 = BeautifulSoup(res2)\n",
    "                    title = soup2.select('article h1')[0].text\n",
    "                    time = soup2.select('article time')[0]['datetime']\n",
    "                    print title, time \n",
    "                    print str_bid\n",
    "                    for p_bid in soup2.select('article p')[0:]:\n",
    "                        print p_bid.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pan_ary = []\n",
    "for \n",
    "    pan_ary.append({\n",
    "        'title':title,\n",
    "        'time':time,\n",
    "        'link':str_bid,\n",
    "        'text':p_bid.text.strip()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "df = pandas.DataFrame(pan_ary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.index.names = \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = soup2.select('article h1')[0].text\n",
    "time = soup2.select('article time')[0]['datetime']\n",
    "print title, time, str_bid\n",
    "for p_bid in soup2.select('article p')[0:]:\n",
    "    print p_bid.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "driver = webdriver.Firefox()\n",
    "ary = ['士林區']    #12個行政區\n",
    "for area in ary:\n",
    "    driver.get('http://www.chinatimes.com/search/result.htm?q=' + area) #Firefox開啟松山區新聞選單頁面\n",
    "    res_text = driver.page_source\n",
    "    soup = BeautifulSoup(res_text)\n",
    "    page_format = 'http://www.chinatimes.com/search/result.htm?q=' + area + '#gsc.tab=0&gsc.q=' + area + '&gsc.page={}'\n",
    "    for page in range(1,3):    #ex:1頁 ==> (1,2)\n",
    "        page_url = page_format.format(page)\n",
    "        driver.get(page_url)\n",
    "        res_text2 = driver.page_source\n",
    "        soup2 = BeautifulSoup(res_text2)\n",
    "        for bid in soup2.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "            if bid.get('href') is not None:\n",
    "                str_bid = str(bid.get('href'))\n",
    "                if 'newspapers' in str_bid:\n",
    "#                      print str_bid                    \n",
    "                    driver.get(str_bid)\n",
    "                    res3 = driver.page_source\n",
    "                    soup3 = BeautifulSoup(res3)\n",
    "                    title = soup3.select('article h1')[0].text\n",
    "                    time = soup3.select('article time')[0]['datetime']\n",
    "                    print title, time \n",
    "                    print str_bid\n",
    "                    for p_bid in soup3.select('article p')[0:]:\n",
    "                        print p_bid.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "driver = webdriver.Firefox()\n",
    "ary = ['士林區']    #12個行政區\n",
    "\n",
    "\n",
    "for area in ary:\n",
    "    page_format = 'http://www.chinatimes.com/search/result.htm?q=' + area + '#gsc.tab=0&gsc.q=' + area + '&gsc.page={}'\n",
    "    for page in range(1,3):    #ex:1頁 ==> (1,2)\n",
    "        page_url = page_format.format(page)\n",
    "        driver.get(page_url)\n",
    "        res_text = driver.page_source\n",
    "        soup = BeautifulSoup(res_text)\n",
    "        for bid in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "            if bid.get('href') is not None:\n",
    "                str_bid = str(bid.get('href'))\n",
    "                if 'newspapers' in str_bid:\n",
    "#                     print str_bid\n",
    "                    driver.get(str_bid)\n",
    "                    res_text2 = driver.page_source\n",
    "                    soup2 = BeautifulSoup(res_text2)\n",
    "                    title = soup3.select('article h1')[0].text\n",
    "                    time = soup3.select('article time')[0]['datetime']\n",
    "                    print title, time \n",
    "                    print str_bid\n",
    "                    for p_bid in soup3.select('article p')[0:]:\n",
    "                        print p_bid.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver.get(str_bid)\n",
    "res_text2 = driver.page_source\n",
    "soup2 = BeautifulSoup(res_text2)\n",
    "print soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = soup3.select('article h1')[0].text\n",
    "time = soup3.select('article time')[0]['datetime']\n",
    "print title, time \n",
    "print str_bid\n",
    "for p_bid in soup3.select('article p')[0:]:\n",
    "    print p_bid.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------start from here-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getlink(soup):\n",
    "    for bid in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "        if bid.get('href') is not None:\n",
    "            str_bid = str(bid.get('href'))\n",
    "            if 'newspapers' in str_bid:\n",
    "                print str_bid\n",
    "                res2 = requests.get(str_bid, headers=headers)\n",
    "                soup2 = BeautifulSoup(res2.content)\n",
    "                res2.close()\n",
    "                time.sleep(3)\n",
    "                for zone in soup2.select('.clear-fix'):\n",
    "                    title = soup2.select('h1')[0].text\n",
    "                    times = soup2.select('time')['datetime']\n",
    "                    text =  soup2.select('p')[0].text.strip() \n",
    "                    print title\n",
    "                    print times\n",
    "                    print area\n",
    "                    print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(2)\n",
    "ary = ['中正區']    #12個行政區\n",
    "url = 'http://www.chinatimes.com/search/result.htm?q='+ary[0]\n",
    "pan_ary = []#建字典\n",
    "for area in ary:\n",
    "    for page in range(1,2):    #ex:1頁 ==> (1,2)\n",
    "        page_url = 'http://www.chinatimes.com/search/result.htm?q=' + area + '#gsc.tab=0&gsc.q=' + area + '&gsc.page={}'.format(page)\n",
    "        \n",
    "        driver.get(page_url)\n",
    "        res_text = driver.page_source\n",
    "        soup = BeautifulSoup(res_text)\n",
    "        \n",
    "        headers = {\n",
    "            'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Encoding':'gzip, deflate, sdch',\n",
    "            'Accept-Language':'zh-TW,zh;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "            'Connection':'keep-alive',\n",
    "            'Host':'www.chinatimes.com',\n",
    "            'Referer':url,\n",
    "            'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'\n",
    "        }\n",
    "        getlink(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.chinatimes.com/newspapers/20150513000138-260204\n",
      "http://www.chinatimes.com/newspapers/20150917000470-260102\n",
      "http://www.chinatimes.com/newspapers/20150326000938-260109\n",
      "http://www.chinatimes.com/newspapers/20150707000640-260107\n",
      "http://www.chinatimes.com/newspapers/20150814000200-260204\n",
      "http://www.chinatimes.com/newspapers/20150706000293-260210\n",
      "http://www.chinatimes.com/newspapers/20150829000452-260107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "\n",
    "ary = ['中正區']    #12個行政區\n",
    "pan_ary = []#建字典\n",
    "for area in ary:\n",
    "    for page in range(1,2):    #ex:1頁 ==> (1,2)\n",
    "        if page == 1:\n",
    "            page_url = \"http://www.chinatimes.com/search/result.htm?q=\"+ area\n",
    "            driver.get(page_url)\n",
    "            res_text = driver.page_source\n",
    "            soup = BeautifulSoup(res_text)\n",
    "            driver.find_element_by_xpath(\"//div[@id='___gcse_0']/div/div/div/div[5]/div[2]/div[2]/div/div[2]/div[13]/div/div[{}]\".format(page+1)).click()\n",
    "        else:\n",
    "            res_text = driver.page_source\n",
    "            soup = BeautifulSoup(res_text)\n",
    "            driver.find_element_by_xpath(\"//div[@id='___gcse_0']/div/div/div/div[5]/div[2]/div[2]/div/div[2]/div[13]/div/div[{}]\".format(page+1)).click()\n",
    "        for bid in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "            if bid.get('href') is not None:\n",
    "                str_bid = str(bid.get('href'))\n",
    "                if 'newspapers' in str_bid:\n",
    "                    print str_bid\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "路平了 大園中正東路 換膚成功\n",
      "2015/08/29 00:00\n",
      "大園區中正東路即縣道110甲線，是大園、中壢往來主要幹道，因連接主要運輸節點，重型車輛往來頻繁，多年下來造成短短4公里道路有80處破損。桃園市工務局自5月底開始替道路「換膚」，原本坑疤的道路變得光滑平整。\n",
      "此次換膚路段為中正東路二、三段，也就是縣道110甲0公里至4.3公里處，沿途有大園交流道、中壢大江購物中心，甚至與多民眾也藉由該道路前往國道3號、桃園機場及桃園高鐵站，都是車潮來往頻繁之處，共計有80處破損。\n",
      "工務局指出，破損處多為車速較高的內側車道車輪行駛處、紅綠燈路口停等區及上下坡綜坡變化轉折處，應屬重車超載、加速或煞車產生的壓力，造路面受到擠壓沉陷所致。\n",
      "市府自今年5月25日開工，7月13日竣工，耗資5184萬元，全面換掉8公分厚的路面，針對嚴重破損處進行下方20至30公分級配層的路基改良，共計改善路基4338平方米、面積共7萬2484平方米，人孔下地212座。\n",
      "桃園市長鄭文燦昨視察換膚路段，他表示，路平是升格後市府團隊主要目標工作，今年編列近30億元預算，預計完成32條縣道及38條鄉道路平工作。(中國時報)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "headers = {\n",
    "            'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Encoding':'gzip, deflate, sdch',\n",
    "            'Accept-Language':'zh-TW,zh;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "            'Connection':'keep-alive',\n",
    "            'Host':'www.chinatimes.com',\n",
    "            'Referer':page_url,\n",
    "            'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'\n",
    "        }\n",
    "\n",
    "res2 = requests.get(str_bid)\n",
    "soup2 = BeautifulSoup(res2.text)\n",
    "res2.close()\n",
    "print soup2.select('article h1')[0].text\n",
    "print soup2.select('article time')[0]['datetime']\n",
    "for p_tag in soup2.select('article p'):\n",
    "    print p_tag.text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "路平了 大園中正東路 換膚成功\n",
      "2015/08/29 00:00\n",
      "大園區中正東路即縣道110甲線，是大園、中壢往來主要幹道，因連接主要運輸節點，重型車輛往來頻繁，多年下來造成短短4公里道路有80處破損。桃園市工務局自5月底開始替道路「換膚」，原本坑疤的道路變得光滑平整。\n",
      "此次換膚路段為中正東路二、三段，也就是縣道110甲0公里至4.3公里處，沿途有大園交流道、中壢大江購物中心，甚至與多民眾也藉由該道路前往國道3號、桃園機場及桃園高鐵站，都是車潮來往頻繁之處，共計有80處破損。\n",
      "工務局指出，破損處多為車速較高的內側車道車輪行駛處、紅綠燈路口停等區及上下坡綜坡變化轉折處，應屬重車超載、加速或煞車產生的壓力，造路面受到擠壓沉陷所致。\n",
      "市府自今年5月25日開工，7月13日竣工，耗資5184萬元，全面換掉8公分厚的路面，針對嚴重破損處進行下方20至30公分級配層的路基改良，共計改善路基4338平方米、面積共7萬2484平方米，人孔下地212座。\n",
      "桃園市長鄭文燦昨視察換膚路段，他表示，路平是升格後市府團隊主要目標工作，今年編列近30億元預算，預計完成32條縣道及38條鄉道路平工作。(中國時報)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-93fc3f4fdb32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'article'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mprint\u001b[0m \u001b[0mele\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'h1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mele\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'datetime'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mp_tag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mele\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0mp_tag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for ele in soup2.select('article'):\n",
    "    print ele.select('h1')[0].text\n",
    "    print ele.select('time')[0]['datetime']\n",
    "    for p_tag in ele.select('p'):\n",
    "        print p_tag.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getinfo(soup):\n",
    "    for bid in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "        if bid.get('href') is not None:\n",
    "            str_bid = str(bid.get('href'))\n",
    "            if 'newspapers' in str_bid:\n",
    "                print str_bid\n",
    "        res2 = requests.get(str_bid)\n",
    "        soup2 = BeautifulSoup(res2.text)\n",
    "        res2.close()\n",
    "        for ele in soup2.select('article'):\n",
    "            print ele.select('h1')[0].text\n",
    "            print ele.select('time')[0]['datetime']\n",
    "            for p_tag in ele.select('p'):\n",
    "                print p_tag.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.chinatimes.com/newspapers/20150513000138-260204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', error(10060, '\\xb3s\\xbdu\\xb9\\xc1\\xb8\\xd5\\xa5\\xa2\\xb1\\xd1\\xa1A\\xa6]\\xac\\xb0\\xb3s\\xbdu\\xb9\\xef\\xb6H\\xa6\\xb3\\xa4@\\xacq\\xae\\xc9\\xb6\\xa1\\xa8\\xc3\\xa5\\xbc\\xa5\\xbf\\xbdT\\xa6^\\xc0\\xb3\\xa1A\\xa9\\xce\\xacO\\xb3s\\xbdu\\xab\\xd8\\xa5\\xdf\\xa5\\xa2\\xb1\\xd1\\xa1A\\xa6]\\xac\\xb0\\xb3s\\xbdu\\xaa\\xba\\xa5D\\xbe\\xf7\\xb5L\\xaak\\xa6^\\xc0\\xb3\\xa1C'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9a8f274930a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;34m'User-Agent'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         }\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mgetinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-8b8226945f06>\u001b[0m in \u001b[0;36mgetinfo\u001b[1;34m(soup)\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;34m'newspapers'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstr_bid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                 \u001b[1;32mprint\u001b[0m \u001b[0mstr_bid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mres2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_bid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0msoup2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mres2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\requests\\api.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\requests\\api.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[1;31m# By explicitly closing the session, we avoid leaving sockets open which\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;31m# can trigger a ResourceWarning in some cases, and look like a memory leak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\requests\\sessions.pyc\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    463\u001b[0m         }\n\u001b[0;32m    464\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\requests\\sessions.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python27\\lib\\site-packages\\requests\\adapters.pyc\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mProtocolError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', error(10060, '\\xb3s\\xbdu\\xb9\\xc1\\xb8\\xd5\\xa5\\xa2\\xb1\\xd1\\xa1A\\xa6]\\xac\\xb0\\xb3s\\xbdu\\xb9\\xef\\xb6H\\xa6\\xb3\\xa4@\\xacq\\xae\\xc9\\xb6\\xa1\\xa8\\xc3\\xa5\\xbc\\xa5\\xbf\\xbdT\\xa6^\\xc0\\xb3\\xa1A\\xa9\\xce\\xacO\\xb3s\\xbdu\\xab\\xd8\\xa5\\xdf\\xa5\\xa2\\xb1\\xd1\\xa1A\\xa6]\\xac\\xb0\\xb3s\\xbdu\\xaa\\xba\\xa5D\\xbe\\xf7\\xb5L\\xaak\\xa6^\\xc0\\xb3\\xa1C'))"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "\n",
    "ary = ['中正區']    #12個行政區\n",
    "pan_ary = []#建字典\n",
    "for area in ary:\n",
    "    for page in range(1,2):    #ex:1頁 ==> (1,2)\n",
    "        if page == 1:\n",
    "            page_url = \"http://www.chinatimes.com/search/result.htm?q=\"+ area\n",
    "            driver.get(page_url)\n",
    "            res_text = driver.page_source\n",
    "            soup = BeautifulSoup(res_text)\n",
    "            driver.find_element_by_xpath(\"//div[@id='___gcse_0']/div/div/div/div[5]/div[2]/div[2]/div/div[2]/div[13]/div/div[{}]\".format(page+1)).click()\n",
    "        else:\n",
    "            res_text = driver.page_source\n",
    "            soup = BeautifulSoup(res_text)\n",
    "            driver.find_element_by_xpath(\"//div[@id='___gcse_0']/div/div/div/div[5]/div[2]/div[2]/div/div[2]/div[13]/div/div[{}]\".format(page+1)).click()\n",
    "        headers = {\n",
    "            'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Encoding':'gzip, deflate, sdch',\n",
    "            'Accept-Language':'zh-TW,zh;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "            'Connection':'keep-alive',\n",
    "            'Host':'www.chinatimes.com',\n",
    "            'Referer':page_url,\n",
    "            'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'\n",
    "        }\n",
    "        getinfo(soup)\n",
    "driver.close()\n",
    "df = pandas.Dataframe(pan_ary)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
