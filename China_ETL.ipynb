{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "driver = webdriver.Firefox()\n",
    "ary = ['士林區']    #12個行政區\n",
    "\n",
    "\n",
    "for area in ary:\n",
    "    page_format = 'http://www.chinatimes.com/search/result.htm?q=' + area + '#gsc.tab=0&gsc.q=' + area + '&gsc.page={}'\n",
    "    for page in range(1,3):    #ex:1頁 ==> (1,2)\n",
    "        page_url = page_format.format(page)\n",
    "        driver.get(page_url)\n",
    "        res_text = driver.page_source\n",
    "        soup = BeautifulSoup(res_text)\n",
    "        for bid in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "            if bid.get('href') is not None:\n",
    "                str_bid = str(bid.get('href'))\n",
    "                if 'newspapers' in str_bid:\n",
    "#                     print str_bid\n",
    "                    driver.get(str_bid)\n",
    "                    res_text2 = driver.page_source\n",
    "                    soup2 = BeautifulSoup(res_text2)\n",
    "                    title = soup3.select('article h1')[0].text\n",
    "                    time = soup3.select('article time')[0]['datetime']\n",
    "                    print title, time \n",
    "                    print str_bid\n",
    "                    for p_bid in soup3.select('article p')[0:]:\n",
    "                        print p_bid.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "driver.get(str_bid)\n",
    "res_text2 = driver.page_source\n",
    "soup2 = BeautifulSoup(res_text2)\n",
    "print soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title = soup3.select('article h1')[0].text\n",
    "time = soup3.select('article time')[0]['datetime']\n",
    "print title, time \n",
    "print str_bid\n",
    "for p_bid in soup3.select('article p')[0:]:\n",
    "    print p_bid.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#----------------------start from here-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getlink(soup):\n",
    "    for bid in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "        if bid.get('href') is not None:\n",
    "            str_bid = str(bid.get('href'))\n",
    "            if 'newspapers' in str_bid:\n",
    "                print str_bid\n",
    "                res2 = requests.get(str_bid, headers=headers)\n",
    "                soup2 = BeautifulSoup(res2.content)\n",
    "                res2.close()\n",
    "                time.sleep(3)\n",
    "                for zone in soup2.select('.clear-fix'):\n",
    "                    title = soup2.select('h1')[0].text\n",
    "                    times = soup2.select('time')['datetime']\n",
    "                    text =  soup2.select('p')[0].text.strip() \n",
    "                    print title\n",
    "                    print times\n",
    "                    print area\n",
    "                    print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(2)\n",
    "ary = ['中正區']    #12個行政區\n",
    "url = 'http://www.chinatimes.com/search/result.htm?q='+ary[0]\n",
    "pan_ary = []#建字典\n",
    "for area in ary:\n",
    "    for page in range(1,2):    #ex:1頁 ==> (1,2)\n",
    "        page_url = 'http://www.chinatimes.com/search/result.htm?q=' + area + '#gsc.tab=0&gsc.q=' + area + '&gsc.page={}'.format(page)\n",
    "        \n",
    "        driver.get(page_url)\n",
    "        res_text = driver.page_source\n",
    "        soup = BeautifulSoup(res_text)\n",
    "        \n",
    "        headers = {\n",
    "            'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Encoding':'gzip, deflate, sdch',\n",
    "            'Accept-Language':'zh-TW,zh;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "            'Connection':'keep-alive',\n",
    "            'Host':'www.chinatimes.com',\n",
    "            'Referer':url,\n",
    "            'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'\n",
    "        }\n",
    "        getlink(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.chinatimes.com/newspapers/20150513000138-260204\n",
      "http://www.chinatimes.com/newspapers/20150917000470-260102\n",
      "http://www.chinatimes.com/newspapers/20150925000727-260110\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "\n",
    "ary = ['中正區']    #12個行政區\n",
    "pan_ary = []#建字典\n",
    "for area in ary:\n",
    "    for page in range(1,2):    #ex:1頁 ==> (1,2)\n",
    "        if page == 1:\n",
    "            page_url = \"http://www.chinatimes.com/search/result.htm?q=\"+ area\n",
    "            driver.get(page_url)\n",
    "            res_text = driver.page_source\n",
    "            soup = BeautifulSoup(res_text)\n",
    "            driver.find_element_by_xpath(\"//div[@id='___gcse_0']/div/div/div/div[5]/div[2]/div[2]/div/div[2]/div[13]/div/div[{}]\".format(page+1)).click()\n",
    "        else:\n",
    "            res_text = driver.page_source\n",
    "            soup = BeautifulSoup(res_text)\n",
    "            driver.find_element_by_xpath(\"//div[@id='___gcse_0']/div/div/div/div[5]/div[2]/div[2]/div/div[2]/div[13]/div/div[{}]\".format(page+1)).click()\n",
    "        for bid in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "            if bid.get('href') is not None:\n",
    "                str_bid = str(bid.get('href'))\n",
    "                if 'newspapers' in str_bid:\n",
    "                    print str_bid\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "headers = {\n",
    "            'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Encoding':'gzip, deflate, sdch',\n",
    "            'Accept-Language':'zh-TW,zh;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "            'Connection':'keep-alive',\n",
    "            'Host':'www.chinatimes.com',\n",
    "            'Referer':page_url,\n",
    "            'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'\n",
    "        }\n",
    "\n",
    "res2 = requests.get(str_bid)\n",
    "soup2 = BeautifulSoup(res2.text)\n",
    "res2.close()\n",
    "# print soup2.select('article h1')[0].text\n",
    "# print soup2.select('article time')[0]['datetime']\n",
    "# for p_tag in soup2.select('article p'):\n",
    "#     print p_tag.text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "畢業季即將到來，社會新鮮人即將踏入職場，但北市昂貴的房租，成為新鮮人生活上一大考驗。專家建議，新鮮人租金預算最好不要超過月薪1／3，以去年畢業生平均薪資27,152元計算，建議應控制在9,000元左右。其中，大同與萬華區就是租屋首選，即使平均租金較高的信義區，也有二線生活圈可選擇。台灣房屋智庫根據租屋實價資訊顯示，緊鄰信義商圈的忠孝東路5段，每月單坪租金1,887元；金融業群聚的南京東路2段租金1,938元。如果以7～8坪套房計算，月租金介於13,200～15,500元間，根本不是初入社會的新鮮人能負擔的價格。究竟9,000元的月租預算，如何在台北市生存？根據《好房網》資料統計，北市套房每坪平均租金最便宜的前5名，分別為文山、北投、大同、士林和萬華區。但文山、北投距離市區過遠，士林區有些地域又靠近山邊，新鮮人想透過捷運轉乘進入台北市區並不方便，生活機能完善也便利的大同、萬華區，才是新鮮人租屋首選。大同區交通便利大同區位於北市西邊，是北市最早發展的區域，周邊有中山地下街、雙連市場、馬偕醫院、寧夏夜市，商業行為成熟，生活機能發達，區內更有捷運圓山站、民權西路站、雙連站、中山站、大橋頭站，交通便捷；但也因為開發早，社區多為老社區，房價、租金相對來說較為便宜。不過，大同區舊屋翻新案例多，又有捷運題材，部分新物建租金水準已飆高，想在大同區找到便宜的房子，還是有些技巧。《好房網》總編輯吳光中建議，新鮮人可往捷運中山站周邊的華陰街、天津街、太原路尋寶，這些路段皆鄰近長安西路且商圈成熟，下班後可覓食的點也多，對於外食族來說相當方便，若以每月租金9,000元左右的預算，可租到7～8坪的房子，是經濟又實惠的選擇。他還指出，捷運民權西路站也是不錯的選擇，除了交通方便、生活機能完善外，治安相對比中山站附近好。他更將建議範圍縮小到以民權西路站為中心，北至天祥路86巷；南至錦西街，這些都是月租9,000元左右的地段。但吳光中叮嚀，租屋不要只看房子，從捷運站到租屋處最好實際走過一遍，特別是女生，走過才知道沿路上是否有監視器，且早晚環境狀況也會不同，建議早晚都走一遍，確認街道巷弄是否會太暗。他強調，年輕女上班族還是要以安全為最重要考量。萬華區注意環境至於位於北市西南側的萬華區，也是大台北最早發展的區域，曾有一府二鹿三艋舺的美譽；如今卻是北市最老舊社區，街廓雜亂無規畫，加上遊民、流鶯聚集，造成一般人對萬華既定印象不佳，房價相對比其他行政區低。不過，萬華區靠近運龍山寺捷運站及西門站的區塊，交通與生活機能都有一定水準。永慶房屋西門店店長楊証舜建議，新鮮人找屋，可朝捷運龍山寺站附近，康定路以東的桂林路上尋找。他指出，該區為純住宅區，治安跟出入分子相對好，如果新鮮人有交通工具，環河南路2段等交通邊陲地帶，也是不錯的選擇。這些地方8坪大的物件，月租金都在1萬元以下。此外，吳光中也建議，如果新鮮人一定要在北市主要辦公商圈租屋，例如大安、信義、松山、中正區，也可以選擇「二線生活圈」。以大安信義區為例，捷運六張犁站附近的信安街、嘉興街、樂利路；或捷運麟光站的臥龍街。至於松山區的二線生活圈，則以捷運南京三民路附近的塔悠路、撫遠街、寶清街、八德路4段為主。專家表示，這些二線生活圈的房租，肯定比行政區的正中心低，新鮮人不妨多關注，或許有機會撈到寶，在距離公司最近的地方租到好房。台北市套房租屋行情（獨立套房）行政區\t每坪平均租金文山區\t1,238.5北投區\t1,267.5大同區\t1,426.5南港區\t1,437.0士林區\t1,465.3萬華區\t1,486.4內湖區\t1,583.9松山區\t1,679.7中山區\t1,679.8中正區\t1,724.4信義區\t1,749.3大安區\t1,928.6新北市主要租屋區行情（獨立套房）行政區\t每坪平均租金板橋區\t1,217.3三重區\t1,203.6中和區\t1,198.0永和區\t1,297.6新莊區\t1,006.3新店區\t1,263.5汐止區\t1033.21946期《時報周刊》一套原價75元，特價59元。全通路雜誌均內附超商折價券，讓讀者激省1093元。伊林娛樂2015璀璨之星選拔即日起開始報名，獎項超豐富，還有機會獲得LUXGEN百萬SUV車。P.S.由「台灣的良心」林杰樑醫師遺孀譚敦慈所書專欄「元氣慈場」於《時報周刊》精采登場，教導民眾如何從生活中杜絕毒物上身。（訂《時報周刊》送健康暢銷書【溫暖腸道，吃出排便力】，請洽讀者服務專線：0800-000-668。）\n"
     ]
    }
   ],
   "source": [
    "# for ele in soup2.select('article'):\n",
    "s =''\n",
    "for p_tag in soup2.select('article p'):\n",
    "    s = s + p_tag.text.strip()\n",
    "# print soup2.select('article header h1')[0].text\n",
    "# print soup2.select('time')[0]['datetime']\n",
    "# for p_tag in soup2.select('article p'):\n",
    "#     print p_tag.text\n",
    "        \n",
    "#     print ele.select('time')[0]['datetime']\n",
    "#     for p_tag in ele.select('p'):\n",
    "#         print p_tag.text\n",
    "print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getinfo(soup):\n",
    "    for bid in soup.select('.gsc-table-cell-thumbnail .gs-title'):\n",
    "        if bid.get('href') is not None:\n",
    "            str_bid = str(bid.get('href'))\n",
    "            if 'newspapers' in str_bid:\n",
    "                print str_bid\n",
    "        res2 = requests.get(str_bid)\n",
    "        soup2 = BeautifulSoup(res2.text)\n",
    "        res2.close()\n",
    "        text = \"\"\n",
    "        for p_tag in soup2.select('article p'):\n",
    "            text = text + p_tag.text.strip()\n",
    "        if title is not None:\n",
    "            title = soup2.select('article header h1')[0].text\n",
    "        title = \"\"\n",
    "        \n",
    "        pan_ary.append({\n",
    "        \"area\":area,\n",
    "        \"links\":str_bid,\n",
    "        \"title\":title,\n",
    "        \"times\":soup2.select('time')[0]['datetime'],\n",
    "        \"content\":text\n",
    "        })            \n",
    "            \n",
    "#             print ele.select('h1')[0].text\n",
    "#             print ele.select('time')[0]['datetime']\n",
    "#             for p_tag in ele.select('p'):\n",
    "#                 print p_tag.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python27\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"html.parser\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'title' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-0fe1840fe8c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;34m'User-Agent'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         }\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mgetinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpan_ary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-61f7a89a58e8>\u001b[0m in \u001b[0;36mgetinfo\u001b[1;34m(soup)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp_tag\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'article p'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mp_tag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'article header h1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'title' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "import pandas\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "\n",
    "ary = ['中正區']    #12個行政區\n",
    "pan_ary = []#建字典\n",
    "for area in ary:\n",
    "    for page in range(1,2):    #ex:1頁 ==> (1,2)\n",
    "        if page == 1:\n",
    "            page_url = \"http://www.chinatimes.com/search/result.htm?q=\"+ area\n",
    "            driver.get(page_url)\n",
    "            driver.find_element_by_css_selector(\"div.gsc-option-selector\").click()\n",
    "            driver.find_element_by_css_selector(\"div.gsc-option\").click()\n",
    "            res_text = driver.page_source\n",
    "            soup = BeautifulSoup(res_text)\n",
    "            driver.find_element_by_xpath(\"//div[@id='___gcse_0']/div/div/div/div[5]/div[2]/div[2]/div/div[2]/div[13]/div/div[{}]\".format(page+1)).click()\n",
    "        else:\n",
    "            res_text = driver.page_source\n",
    "            soup = BeautifulSoup(res_text)\n",
    "            driver.find_element_by_xpath(\"//div[@id='___gcse_0']/div/div/div/div[5]/div[2]/div[2]/div/div[2]/div[13]/div/div[{}]\".format(page+1)).click()\n",
    "        headers = {\n",
    "            'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Encoding':'gzip, deflate, sdch',\n",
    "            'Accept-Language':'zh-TW,zh;q=0.8,en-US;q=0.6,en;q=0.4',\n",
    "            'Connection':'keep-alive',\n",
    "            'Host':'www.chinatimes.com',\n",
    "            'Referer':page_url,\n",
    "            'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'\n",
    "        }\n",
    "        getinfo(soup)\n",
    "driver.close()\n",
    "df = pandas.Dataframe(pan_ary)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import NoAlertPresentException\n",
    "import unittest, time, re\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.implicitly_wait(30)\n",
    "base_url = \"http://www.chinatimes.com/\"\n",
    "driver.get(base_url + \"/search/result.htm?q=%E5%A3%AB%E6%9E%97%E5%8D%80#gsc.tab=0&gsc.q=%E5%A3%AB%E6%9E%97%E5%8D%80&gsc.page=1\")\n",
    "driver.find_element_by_css_selector(\"div.gsc-option-selector\").click()\n",
    "driver.find_element_by_css_selector(\"div.gsc-option\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# a = 3\n",
    "a = \"\"\n",
    "\n",
    "if a is None:\n",
    "    print \"null\"\n",
    "else:\n",
    "    print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
